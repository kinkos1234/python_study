# íŠœí† ë¦¬ì–¼ 5: ê³¼ì†Œì í•©ê³¼ ê³¼ëŒ€ì í•© (Underfitting and Overfitting)

> ì›ë³¸: [Kaggle - Underfitting and Overfitting](https://www.kaggle.com/code/dansbecker/underfitting-and-overfitting)  
> ì‹¤ìŠµ: [Ex 4: ê³¼ì†Œ/ê³¼ëŒ€ì í•©](../exercises/ex4_underfitting_overfitting.ipynb)

---

## ğŸ¯ í•™ìŠµ ëª©í‘œ

- ê³¼ëŒ€ì í•©(Overfitting)ê³¼ ê³¼ì†Œì í•©(Underfitting)ì˜ ê°œë…ì„ ì´í•´í•œë‹¤
- `max_leaf_nodes` íŒŒë¼ë¯¸í„°ë¡œ ìµœì ì˜ ëª¨ë¸ ë³µì¡ë„ë¥¼ ì°¾ëŠ”ë‹¤

---

## ğŸ“– ë‚´ìš© ì •ë¦¬

### ë‹¤ì–‘í•œ ëª¨ë¸ ì‹¤í—˜í•˜ê¸°

ì´ì „ íŠœí† ë¦¬ì–¼ì—ì„œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ í‰ê°€ ë°©ë²•(ê²€ì¦ ë°ì´í„°)ì„ ë°°ì› ìŠµë‹ˆë‹¤. ì´ì œ **ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì‹¤í—˜**í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê²°ì • íŠ¸ë¦¬ëŠ” **íŠ¸ë¦¬ì˜ ê¹Šì´(depth)** ë¥¼ ì¡°ì ˆí•˜ëŠ” ì—¬ëŸ¬ ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤.

---

### ê³¼ëŒ€ì í•© (Overfitting)

íŠ¸ë¦¬ê°€ **ë„ˆë¬´ ê¹Šìœ¼ë©´** ì–´ë–»ê²Œ ë ê¹Œìš”?

- ë°ì´í„°ê°€ ë§¤ìš° ì„¸ë°€í•˜ê²Œ ë¶„ë¥˜ë¨ â†’ ê° ë¦¬í”„(leaf)ì— ì•„ì£¼ ì ì€ ì§‘ë§Œ í¬í•¨
- í›ˆë ¨ ë°ì´í„°ì—ì„œëŠ” ê±°ì˜ ì™„ë²½í•œ ì˜ˆì¸¡ (ë¦¬í”„ì— ìˆëŠ” ì§‘ë§Œ í•™ìŠµí–ˆìœ¼ë‹ˆê¹Œ)
- ê·¸ëŸ¬ë‚˜ **ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œëŠ” í˜•í¸ì—†ëŠ” ì„±ëŠ¥**

ì´ë¥¼ **ê³¼ëŒ€ì í•©(Overfitting)** ì´ë¼ê³  í•©ë‹ˆë‹¤:

> ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ë¥¼ ë„ˆë¬´ ì˜ ì™¸ì›Œë²„ë ¤ì„œ, ìƒˆë¡œìš´ ë°ì´í„°ì— ì¼ë°˜í™”ë˜ì§€ ëª»í•˜ëŠ” í˜„ìƒ

---

### ê³¼ì†Œì í•© (Underfitting)

íŠ¸ë¦¬ê°€ **ë„ˆë¬´ ì–•ìœ¼ë©´** ì–´ë–»ê²Œ ë ê¹Œìš”?

- ë°ì´í„°ê°€ ê±°ì¹ ê²Œ ë¶„ë¥˜ë¨ â†’ ê° ê·¸ë£¹ì— ë§¤ìš° ë‹¤ì–‘í•œ ì§‘ì´ í¬í•¨
- í›ˆë ¨ ë°ì´í„°ì—ì„œë„ ì˜ˆì¸¡ì´ ë¶€ì •í™•
- ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œë„ ë‹¹ì—°íˆ ë¶€ì •í™•

ì´ë¥¼ **ê³¼ì†Œì í•©(Underfitting)** ì´ë¼ê³  í•©ë‹ˆë‹¤:

> ëª¨ë¸ì´ ë°ì´í„°ì˜ ì¤‘ìš”í•œ íŒ¨í„´ì„ í¬ì°©í•˜ì§€ ëª»í•´ì„œ, í›ˆë ¨ ë°ì´í„°ì—ì„œë„ ì„±ëŠ¥ì´ ë‚®ì€ í˜„ìƒ

---

### ìŠ¤ìœ„íŠ¸ ìŠ¤íŒŸ ì°¾ê¸° (Sweet Spot)

```
ì˜¤      â”‚           í›ˆë ¨ ì˜¤ì°¨ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
ì°¨      â”‚  ï¼¼
(MAE)  â”‚   ï¼¼                      ê²€ì¦ ì˜¤ì°¨
        â”‚    ï¼¼              â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚     ï¼¼           â•±
        â”‚      ï¼¼         â•±
        â”‚       ï¼¼       â•±
        â”‚        ï¼¼_____â•±   â† ìŠ¤ìœ„íŠ¸ ìŠ¤íŒŸ! (ìµœì )
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
             ëª¨ë¸ ë³µì¡ë„ (íŠ¸ë¦¬ ê¹Šì´/ë¦¬í”„ ìˆ˜)
           ë‚®ìŒ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºë†’ìŒ
          (ê³¼ì†Œì í•©)                   (ê³¼ëŒ€ì í•©)
```

ìš°ë¦¬ëŠ” **ê²€ì¦ ì˜¤ì°¨ê°€ ê°€ì¥ ë‚®ì€ ì§€ì **ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.

---

### `max_leaf_nodes`ë¡œ ìµœì  ëª¨ë¸ ì°¾ê¸°

`max_leaf_nodes` íŒŒë¼ë¯¸í„°ë¡œ íŠ¸ë¦¬ì˜ ë¦¬í”„ ìˆ˜ë¥¼ ì œí•œí•˜ì—¬ ë³µì¡ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.

#### MAE ë¹„êµ í•¨ìˆ˜ ì •ì˜

```python
from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor

def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
    model.fit(train_X, train_y)
    preds_val = model.predict(val_X)
    mae = mean_absolute_error(val_y, preds_val)
    return mae
```

#### ì—¬ëŸ¬ ê°’ ë¹„êµ

```python
# ë‹¤ì–‘í•œ max_leaf_nodes ê°’ì— ëŒ€í•´ MAE ê³„ì‚°
for max_leaf_nodes in [5, 50, 500, 5000]:
    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)
    print(f"ë¦¬í”„ ìµœëŒ€ {max_leaf_nodes}ê°œ: MAE = {my_mae:,.0f}")
```

ì˜ˆì‹œ ê²°ê³¼:

| max_leaf_nodes | MAE          | ìƒíƒœ          |
| -------------- | ------------ | ------------- |
| 5              | ~347,000     | ê³¼ì†Œì í•©      |
| 50             | ~258,000     | ê°œì„ ë¨        |
| 500            | **~243,000** | **ìµœì !**     |
| 5000           | ~254,000     | ê³¼ëŒ€ì í•© ì‹œì‘ |

#### ìµœì  ëª¨ë¸ë¡œ ìµœì¢… í•™ìŠµ

```python
# ì „ì²´ ë°ì´í„°ë¡œ ìµœì  íŒŒë¼ë¯¸í„° ëª¨ë¸ ì¬í•™ìŠµ (ê²€ì¦ ë¶„ë¦¬ ì—†ì´)
final_model = DecisionTreeRegressor(max_leaf_nodes=500, random_state=0)
final_model.fit(X, y)
```

---

## ğŸ’» ì „ì²´ ì½”ë“œ

```python
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# ë°ì´í„° ë¡œë“œ
melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path)
filtered_melbourne_data = melbourne_data.dropna(axis=0)

y = filtered_melbourne_data.Price
melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea',
                      'YearBuilt', 'Lattitude', 'Longtitude']
X = filtered_melbourne_data[melbourne_features]

train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)

# MAE ë¹„êµ í•¨ìˆ˜
def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
    model.fit(train_X, train_y)
    preds_val = model.predict(val_X)
    return mean_absolute_error(val_y, preds_val)

# ìµœì  max_leaf_nodes ì°¾ê¸°
scores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y)
          for leaf_size in [5, 25, 50, 100, 250, 500]}
best_tree_size = min(scores, key=scores.get)
print(f"ìµœì  ë¦¬í”„ ìˆ˜: {best_tree_size}")

# ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
final_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=0)
final_model.fit(X, y)
```

---

## ğŸ’¡ í•µì‹¬ ì •ë¦¬

| ê°œë…         | ì¦ìƒ          | í•´ê²°ì±…                   |
| ------------ | ------------- | ------------------------ |
| **ê³¼ëŒ€ì í•©** | í›ˆë ¨â†‘ ê²€ì¦â†“   | íŠ¸ë¦¬ ê¹Šì´/ë¦¬í”„ ìˆ˜ ì¤„ì´ê¸° |
| **ê³¼ì†Œì í•©** | í›ˆë ¨â†“ ê²€ì¦â†“   | íŠ¸ë¦¬ ê¹Šì´/ë¦¬í”„ ìˆ˜ ëŠ˜ë¦¬ê¸° |
| **ìµœì **     | ê²€ì¦ MAE ìµœì†Œ | `max_leaf_nodes` íŠœë‹    |

---

## â¡ï¸ ë‹¤ìŒ ë‹¨ê³„

- ğŸ“ [ì‹¤ìŠµ 4: ê³¼ì†Œ/ê³¼ëŒ€ì í•©](../exercises/ex4_underfitting_overfitting.ipynb) ì™„ë£Œ í›„
- [íŠœí† ë¦¬ì–¼ 6: ëœë¤ í¬ë ˆìŠ¤íŠ¸](06_random_forests.md)ë¡œ ì´ë™
